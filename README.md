
# ðŸ§  Transformer Architecture Notes  

This repository contains my detailed study notes on the **Transformer Architecture** â€“ the backbone of modern NLP and Large Language Models (LLMs) like BERT, GPT, and T5.  
The goal of this repo is to **document my learning journey** and provide a structured reference for revision and sharing.  

---

## ðŸ“‘ Contents  

- ðŸ”¹ **Introduction to Transformers**  
  - Why Transformers replaced RNNs & LSTMs  
  - Advantages over sequential models  

- ðŸ”¹ **Self-Attention Mechanism**  
  - Queries, Keys, Values (Q, K, V)  
  - Scaled Dot-Product Attention  

- ðŸ”¹ **Multi-Head Attention**  
  - Parallel attention heads  
  - Learning multiple representation subspaces  

- ðŸ”¹ **Positional Encoding**  
  - Injecting order into input sequences  
  - Sine and cosine functions  

- ðŸ”¹ **Encoder-Decoder Architecture**  
  - Layer breakdown  
  - Role of encoders vs. decoders  

- ðŸ”¹ **Applications**  
  - Machine Translation  
  - Text Summarisation  
  - BERT, GPT, and other LLMs
 


- ðŸ”¹ **Refrences**
  - Vaswani et al. â€“ "Attention Is All You Need" (2017)
  
      

```math
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
